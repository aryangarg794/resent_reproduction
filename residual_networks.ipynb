{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduction of *Deep Residual Learning for Image Recognition*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper here: https://arxiv.org/pdf/1512.03385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_init(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlockConv(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            in_channels=None,\n",
    "            out_channels=None,\n",
    "            num_layers=2, \n",
    "            kernel_size=3,\n",
    "            projection=False,\n",
    "            activation=nn.ReLU()\n",
    "    ) -> None:\n",
    "        super(ResidualBlockConv, self).__init__()\n",
    "        assert in_channels is not None, \"Input channels is set to None\"\n",
    "        assert out_channels is not None, \"Output channels is set to None\"\n",
    "\n",
    "        self.projection = projection\n",
    "        self.layers = nn.Sequential()\n",
    "        ÃŸ\n",
    "        if self.projection:\n",
    "            self.downsample_conv = nn.Conv2d(\n",
    "                kernel_size=1, \n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                stride=2\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        for i in range(num_layers+1):\n",
    "            if i == 0 and self.projection:\n",
    "                layers.append(nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=1,\n",
    "                    stride=2\n",
    "                ))\n",
    "            elif i == num_layers - 1:\n",
    "                layers.append(activation)\n",
    "            else:\n",
    "                layers.append(nn.Conv2d(\n",
    "                    in_channels=out_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=1\n",
    "                ))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.last_relu = activation\n",
    "\n",
    "\n",
    "    def downsample(self, x):\n",
    "        if self.downsample_conv is not None:\n",
    "            return self.downsample_conv(x)\n",
    "        return x\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.layers(x)\n",
    "             \n",
    "        # check if we need to downsample\n",
    "        if self.projection:\n",
    "             residual = self.downsample(residual)\n",
    "        \n",
    "        # perform the residual mapping\n",
    "        out += residual\n",
    "        out = self.last_relu(out)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet34(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            net_blocks,\n",
    "            num_classes=10,\n",
    "            activation=nn.ReLU(),\n",
    "        ) -> None:\n",
    "        super(ResNet34, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=64,\n",
    "            kernel_size=7,\n",
    "            stride=2,\n",
    "            padding=3\n",
    "        ))\n",
    "\n",
    "        self.layers.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "        self.layers.append(activation)\n",
    "        self.layers.append(self._make_block(net_blocks[0], 64, 64))\n",
    "        self.layers.append(self._make_block(net_blocks[1], 64, 128))\n",
    "        self.layers.append(self._make_block(net_blocks[2], 128, 256))\n",
    "        self.layers.append(self._make_block(net_blocks[3], 256, 512))\n",
    "        self.layers.append(nn.AdaptiveAvgPool2d(output_size=(1, 1)))\n",
    "        self.layers.append(nn.Flatten(start_dim=1))\n",
    "        self.layers.append(nn.Linear(in_features=512, out_features=num_classes))\n",
    "\n",
    "        self.apply(kaiming_init)\n",
    "    \n",
    "    def _make_block(self, num_blocks, in_channels, out_channels):\n",
    "        new_block = nn.Sequential()\n",
    "        for i in range(num_blocks):\n",
    "            if in_channels != out_channels:\n",
    "                if i == 0:\n",
    "                    new_block.append(ResidualBlockConv(\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=out_channels,\n",
    "                        projection=True\n",
    "                    ))\n",
    "                else:\n",
    "                    new_block.append(ResidualBlockConv(\n",
    "                        in_channels=out_channels,\n",
    "                        out_channels=out_channels\n",
    "                    ))\n",
    "            else:\n",
    "                new_block.append(ResidualBlockConv(\n",
    "                    in_channels=out_channels,\n",
    "                    out_channels=out_channels\n",
    "                ))\n",
    "\n",
    "            return new_block\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        return out \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_resnet = ResNet34([3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [32, 64, 16, 16]           9,472\n",
      "         MaxPool2d-2             [32, 64, 8, 8]               0\n",
      "              ReLU-3             [32, 64, 8, 8]               0\n",
      "            Conv2d-4             [32, 64, 8, 8]          36,928\n",
      "              ReLU-5             [32, 64, 8, 8]               0\n",
      "              ReLU-6             [32, 64, 8, 8]               0\n",
      "              ReLU-7             [32, 64, 8, 8]               0\n",
      "              ReLU-8             [32, 64, 8, 8]               0\n",
      "              ReLU-9             [32, 64, 8, 8]               0\n",
      "             ReLU-10             [32, 64, 8, 8]               0\n",
      "             ReLU-11             [32, 64, 8, 8]               0\n",
      "             ReLU-12             [32, 64, 8, 8]               0\n",
      "           Conv2d-13             [32, 64, 8, 8]          36,928\n",
      "             ReLU-14             [32, 64, 8, 8]               0\n",
      "             ReLU-15             [32, 64, 8, 8]               0\n",
      "             ReLU-16             [32, 64, 8, 8]               0\n",
      "             ReLU-17             [32, 64, 8, 8]               0\n",
      "             ReLU-18             [32, 64, 8, 8]               0\n",
      "             ReLU-19             [32, 64, 8, 8]               0\n",
      "             ReLU-20             [32, 64, 8, 8]               0\n",
      "             ReLU-21             [32, 64, 8, 8]               0\n",
      "ResidualBlockConv-22             [32, 64, 8, 8]               0\n",
      "           Conv2d-23            [32, 128, 4, 4]          73,856\n",
      "             ReLU-24            [32, 128, 4, 4]               0\n",
      "             ReLU-25            [32, 128, 4, 4]               0\n",
      "             ReLU-26            [32, 128, 4, 4]               0\n",
      "             ReLU-27            [32, 128, 4, 4]               0\n",
      "             ReLU-28            [32, 128, 4, 4]               0\n",
      "             ReLU-29            [32, 128, 4, 4]               0\n",
      "             ReLU-30            [32, 128, 4, 4]               0\n",
      "             ReLU-31            [32, 128, 4, 4]               0\n",
      "           Conv2d-32            [32, 128, 4, 4]         147,584\n",
      "           Conv2d-33            [32, 128, 4, 4]           8,320\n",
      "             ReLU-34            [32, 128, 4, 4]               0\n",
      "             ReLU-35            [32, 128, 4, 4]               0\n",
      "             ReLU-36            [32, 128, 4, 4]               0\n",
      "             ReLU-37            [32, 128, 4, 4]               0\n",
      "             ReLU-38            [32, 128, 4, 4]               0\n",
      "             ReLU-39            [32, 128, 4, 4]               0\n",
      "             ReLU-40            [32, 128, 4, 4]               0\n",
      "             ReLU-41            [32, 128, 4, 4]               0\n",
      "ResidualBlockConv-42            [32, 128, 4, 4]               0\n",
      "           Conv2d-43            [32, 256, 2, 2]         295,168\n",
      "             ReLU-44            [32, 256, 2, 2]               0\n",
      "             ReLU-45            [32, 256, 2, 2]               0\n",
      "             ReLU-46            [32, 256, 2, 2]               0\n",
      "             ReLU-47            [32, 256, 2, 2]               0\n",
      "             ReLU-48            [32, 256, 2, 2]               0\n",
      "             ReLU-49            [32, 256, 2, 2]               0\n",
      "             ReLU-50            [32, 256, 2, 2]               0\n",
      "             ReLU-51            [32, 256, 2, 2]               0\n",
      "           Conv2d-52            [32, 256, 2, 2]         590,080\n",
      "           Conv2d-53            [32, 256, 2, 2]          33,024\n",
      "             ReLU-54            [32, 256, 2, 2]               0\n",
      "             ReLU-55            [32, 256, 2, 2]               0\n",
      "             ReLU-56            [32, 256, 2, 2]               0\n",
      "             ReLU-57            [32, 256, 2, 2]               0\n",
      "             ReLU-58            [32, 256, 2, 2]               0\n",
      "             ReLU-59            [32, 256, 2, 2]               0\n",
      "             ReLU-60            [32, 256, 2, 2]               0\n",
      "             ReLU-61            [32, 256, 2, 2]               0\n",
      "ResidualBlockConv-62            [32, 256, 2, 2]               0\n",
      "           Conv2d-63            [32, 512, 1, 1]       1,180,160\n",
      "             ReLU-64            [32, 512, 1, 1]               0\n",
      "             ReLU-65            [32, 512, 1, 1]               0\n",
      "             ReLU-66            [32, 512, 1, 1]               0\n",
      "             ReLU-67            [32, 512, 1, 1]               0\n",
      "             ReLU-68            [32, 512, 1, 1]               0\n",
      "             ReLU-69            [32, 512, 1, 1]               0\n",
      "             ReLU-70            [32, 512, 1, 1]               0\n",
      "             ReLU-71            [32, 512, 1, 1]               0\n",
      "           Conv2d-72            [32, 512, 1, 1]       2,359,808\n",
      "           Conv2d-73            [32, 512, 1, 1]         131,584\n",
      "             ReLU-74            [32, 512, 1, 1]               0\n",
      "             ReLU-75            [32, 512, 1, 1]               0\n",
      "             ReLU-76            [32, 512, 1, 1]               0\n",
      "             ReLU-77            [32, 512, 1, 1]               0\n",
      "             ReLU-78            [32, 512, 1, 1]               0\n",
      "             ReLU-79            [32, 512, 1, 1]               0\n",
      "             ReLU-80            [32, 512, 1, 1]               0\n",
      "             ReLU-81            [32, 512, 1, 1]               0\n",
      "ResidualBlockConv-82            [32, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-83            [32, 512, 1, 1]               0\n",
      "          Flatten-84                  [32, 512]               0\n",
      "           Linear-85                  [32, 100]          51,300\n",
      "       LogSoftmax-86                  [32, 100]               0\n",
      "================================================================\n",
      "Total params: 4,954,212\n",
      "Trainable params: 4,954,212\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.38\n",
      "Forward/backward pass size (MB): 42.80\n",
      "Params size (MB): 18.90\n",
      "Estimated Total Size (MB): 62.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(test_resnet, input_size=(3, 32, 32), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a plain network to constrast the residual learning net with a plain deep one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plain34(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes=10,\n",
    "            activation=nn.ReLU()\n",
    "        ) -> None:\n",
    "        super(Plain34, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=64,\n",
    "            kernel_size=7,\n",
    "            stride=2,\n",
    "            padding=3\n",
    "        ))\n",
    "\n",
    "        self.layers.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "        self.layers.append(activation)\n",
    "\n",
    "        for _ in range(6):\n",
    "            self.layers.append(nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                padding=1\n",
    "            ))\n",
    "            self.layers.append(activation)\n",
    "        \n",
    "        self.layers.append(nn.Conv2d(\n",
    "            in_channels=64, \n",
    "            out_channels=128,\n",
    "            kernel_size=3, \n",
    "            stride=2,\n",
    "            padding=1\n",
    "        ))\n",
    "        self.layers.append(activation)\n",
    "\n",
    "        for _ in range(7):\n",
    "            self.layers.append(nn.Conv2d(\n",
    "                in_channels=128,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                padding=1\n",
    "            ))\n",
    "            self.layers.append(activation)\n",
    "        \n",
    "        self.layers.append(nn.Conv2d(\n",
    "            in_channels=128, \n",
    "            out_channels=256,\n",
    "            kernel_size=3, \n",
    "            stride=2,\n",
    "            padding=1\n",
    "        ))\n",
    "        self.layers.append(activation)\n",
    "\n",
    "        for _ in range(11):\n",
    "            self.layers.append(nn.Conv2d(\n",
    "                in_channels=256,\n",
    "                out_channels=256,\n",
    "                kernel_size=3,\n",
    "                padding=1\n",
    "            ))\n",
    "            self.layers.append(activation)\n",
    "\n",
    "        self.layers.append(nn.Conv2d(\n",
    "            in_channels=256, \n",
    "            out_channels=512,\n",
    "            kernel_size=3, \n",
    "            stride=2,\n",
    "            padding=1\n",
    "        ))\n",
    "        self.layers.append(activation)\n",
    "\n",
    "        for _ in range(5):\n",
    "            self.layers.append(nn.Conv2d(\n",
    "                in_channels=512,\n",
    "                out_channels=512,\n",
    "                kernel_size=3,\n",
    "                padding=1\n",
    "            ))\n",
    "            self.layers.append(activation)\n",
    "\n",
    "        self.layers.append(nn.AdaptiveAvgPool2d(\n",
    "            output_size=(1, 1)\n",
    "        ))\n",
    "\n",
    "        self.layers.append(nn.Flatten(start_dim=1))\n",
    "\n",
    "        self.layers.append(nn.Linear(\n",
    "            in_features=512,\n",
    "            out_features=num_classes\n",
    "        ))\n",
    "        \n",
    "        # self.layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "        self.apply(kaiming_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x \n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_net = Plain34()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [32, 64, 16, 16]           9,472\n",
      "         MaxPool2d-2             [32, 64, 8, 8]               0\n",
      "              ReLU-3             [32, 64, 8, 8]               0\n",
      "            Conv2d-4             [32, 64, 8, 8]          36,928\n",
      "              ReLU-5             [32, 64, 8, 8]               0\n",
      "            Conv2d-6             [32, 64, 8, 8]          36,928\n",
      "              ReLU-7             [32, 64, 8, 8]               0\n",
      "            Conv2d-8             [32, 64, 8, 8]          36,928\n",
      "              ReLU-9             [32, 64, 8, 8]               0\n",
      "           Conv2d-10             [32, 64, 8, 8]          36,928\n",
      "             ReLU-11             [32, 64, 8, 8]               0\n",
      "           Conv2d-12             [32, 64, 8, 8]          36,928\n",
      "             ReLU-13             [32, 64, 8, 8]               0\n",
      "           Conv2d-14             [32, 64, 8, 8]          36,928\n",
      "             ReLU-15             [32, 64, 8, 8]               0\n",
      "           Conv2d-16            [32, 128, 4, 4]          73,856\n",
      "             ReLU-17            [32, 128, 4, 4]               0\n",
      "           Conv2d-18            [32, 128, 4, 4]         147,584\n",
      "             ReLU-19            [32, 128, 4, 4]               0\n",
      "           Conv2d-20            [32, 128, 4, 4]         147,584\n",
      "             ReLU-21            [32, 128, 4, 4]               0\n",
      "           Conv2d-22            [32, 128, 4, 4]         147,584\n",
      "             ReLU-23            [32, 128, 4, 4]               0\n",
      "           Conv2d-24            [32, 128, 4, 4]         147,584\n",
      "             ReLU-25            [32, 128, 4, 4]               0\n",
      "           Conv2d-26            [32, 128, 4, 4]         147,584\n",
      "             ReLU-27            [32, 128, 4, 4]               0\n",
      "           Conv2d-28            [32, 128, 4, 4]         147,584\n",
      "             ReLU-29            [32, 128, 4, 4]               0\n",
      "           Conv2d-30            [32, 128, 4, 4]         147,584\n",
      "             ReLU-31            [32, 128, 4, 4]               0\n",
      "           Conv2d-32            [32, 256, 2, 2]         295,168\n",
      "             ReLU-33            [32, 256, 2, 2]               0\n",
      "           Conv2d-34            [32, 256, 2, 2]         590,080\n",
      "             ReLU-35            [32, 256, 2, 2]               0\n",
      "           Conv2d-36            [32, 256, 2, 2]         590,080\n",
      "             ReLU-37            [32, 256, 2, 2]               0\n",
      "           Conv2d-38            [32, 256, 2, 2]         590,080\n",
      "             ReLU-39            [32, 256, 2, 2]               0\n",
      "           Conv2d-40            [32, 256, 2, 2]         590,080\n",
      "             ReLU-41            [32, 256, 2, 2]               0\n",
      "           Conv2d-42            [32, 256, 2, 2]         590,080\n",
      "             ReLU-43            [32, 256, 2, 2]               0\n",
      "           Conv2d-44            [32, 256, 2, 2]         590,080\n",
      "             ReLU-45            [32, 256, 2, 2]               0\n",
      "           Conv2d-46            [32, 256, 2, 2]         590,080\n",
      "             ReLU-47            [32, 256, 2, 2]               0\n",
      "           Conv2d-48            [32, 256, 2, 2]         590,080\n",
      "             ReLU-49            [32, 256, 2, 2]               0\n",
      "           Conv2d-50            [32, 256, 2, 2]         590,080\n",
      "             ReLU-51            [32, 256, 2, 2]               0\n",
      "           Conv2d-52            [32, 256, 2, 2]         590,080\n",
      "             ReLU-53            [32, 256, 2, 2]               0\n",
      "           Conv2d-54            [32, 256, 2, 2]         590,080\n",
      "             ReLU-55            [32, 256, 2, 2]               0\n",
      "           Conv2d-56            [32, 512, 1, 1]       1,180,160\n",
      "             ReLU-57            [32, 512, 1, 1]               0\n",
      "           Conv2d-58            [32, 512, 1, 1]       2,359,808\n",
      "             ReLU-59            [32, 512, 1, 1]               0\n",
      "           Conv2d-60            [32, 512, 1, 1]       2,359,808\n",
      "             ReLU-61            [32, 512, 1, 1]               0\n",
      "           Conv2d-62            [32, 512, 1, 1]       2,359,808\n",
      "             ReLU-63            [32, 512, 1, 1]               0\n",
      "           Conv2d-64            [32, 512, 1, 1]       2,359,808\n",
      "             ReLU-65            [32, 512, 1, 1]               0\n",
      "           Conv2d-66            [32, 512, 1, 1]       2,359,808\n",
      "             ReLU-67            [32, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-68            [32, 512, 1, 1]               0\n",
      "          Flatten-69                  [32, 512]               0\n",
      "           Linear-70                   [32, 10]           5,130\n",
      "       LogSoftmax-71                   [32, 10]               0\n",
      "================================================================\n",
      "Total params: 21,108,362\n",
      "Trainable params: 21,108,362\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.38\n",
      "Forward/backward pass size (MB): 33.75\n",
      "Params size (MB): 80.52\n",
      "Estimated Total Size (MB): 114.65\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(plain_net, input_size = (3, 32, 32), batch_size= 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import v2\n",
    "from yaml import load, Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as f:\n",
    "    config = load(f, Loader)\n",
    "\n",
    "num_classs = config['num_classes']\n",
    "starting_lr = config['starting_lr']\n",
    "input_size = config['input_size']\n",
    "epochs = config['epochs']\n",
    "batch_size = config['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation_train = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomRotation(degrees=90),\n",
    "    v2.ColorJitter(brightness=.5, hue=.3),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation_test = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_train = CIFAR10('data/', train=True, transform=transformation_train, download=True)\n",
    "cifar10_test = CIFAR10('data/', transform=transformation_test, download=True)\n",
    "\n",
    "trainloader = DataLoader(cifar10_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(cifar10_test, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(plain_net.parameters(), lr=starting_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[339], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mplain_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/Projects/residual-learning/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/residual-learning/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[297], line 102\u001b[0m, in \u001b[0;36mPlain34.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    100\u001b[0m out \u001b[38;5;241m=\u001b[39m x \n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 102\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Documents/Projects/residual-learning/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/residual-learning/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/residual-learning/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:104\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/residual-learning/venv/lib/python3.11/site-packages/torch/nn/functional.py:1500\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = plain_net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # repurposed from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#define-a-loss-function-and-optimizer\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    \n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "print(running_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
